# ProseGenerator â€” Fullâ€‘Stack AI Agent

LangGraphâ€‘powered **AI writing assistant** that turns a short list of *beats* into a â‰ˆ1â€¯500â€‘word passage **and** (optionally) illustrates the chapter with an AIâ€‘generated cover.\
The agent selfâ€‘critiques its prose and image via Judgeâ€‘val.\
A lightweight React UI (bootâ€‘strapped with **Create React App**, *not*Â Vite) provides a oneâ€‘click playground.

---

## 0Â Â·Â TechÂ StackÂ & Key Libraries

| Layer                 | LibraryÂ / Service                            | Purpose                                                        |
| --------------------- | -------------------------------------------- | -------------------------------------------------------------- |
| **Agent graph**       | **LangGraph**                                | Declarative DAG & stateâ€‘machine orchestrating every step.      |
| LLM access            | **OpenAIÂ PythonÂ SDK**Â +Â **LangChainâ€‘OpenAI** | Chat, embeddings & GPTâ€‘Image calls.                            |
| Vector store          | **Pinecone**                                 | RAG retrieval: Booksâ€‘um index â†’ semantic context snippets.     |
| Reâ€‘ranking            | **Sentenceâ€‘TransformersÂ Crossâ€‘Encoder**      | Reranks Pinecone hits for higher relevance.                    |
| Persistence / tracing | **Judgeâ€‘valÂ Tracer**                         | Fineâ€‘grained traces **plus** relevancy scoring (text & image). |
| API server            | **FastAPI**Â +Â **Uvicorn**                    | Exposes `/generate-prose` & static cover assets.               |
| Web UI                | **ReactÂ 18** (CreateÂ ReactÂ App)              | Simple form & result viewer (noÂ Vite).                         |

Other helpers: `pythonâ€‘dotenv`, `torch`, `requests`, `pillow`, etc.

> **GPU?** Optional. The conda env installs CPU wheels by default.

---

## 1Â Â·Â Prerequisites

| Tool             | VersionÂ tested | Notes                          |
| ---------------- | -------------- | ------------------------------ |
| Python           | 3.12           | Managed via **conda**.         |
| conda /Â Mamba    | â‰¥â€¯23           | `conda --version`              |
| Git              | any            | Clone repo.                    |
| C/C++ toolâ€‘chain | system default | Needed by a few Python wheels. |

---

## 2Â Â·Â Clone the Repository

```bash
git clone https://github.com/andresca94/Prose-Art-Agent.git
cd Prose-Art-Agent
```

---

## 3 Â· Provision the Backend Environment

A preâ€‘built `environment.yml` is already committed at the repo rootâ€”no need to create it manually.

```bash
# from repo root
conda env create -f environment.yml
conda activate prose-art-gen
```

This pulls PythonÂ 3.12 plus every backend dependency (FastAPI, LangGraph, Pinecone, Judgeâ€‘val, etc.).
If you require a GPU build of **PyTorch**, install the appropriate CUDA/MPS wheel *before* running the commands above, then recreate the env.

---

## 4Â Â·Â Project LayoutÂ Â·Â Project Layout

```
ProseGenerator/
â”œâ”€â”€ backend/          # FastAPI + LangGraph agent
â”‚   â”œâ”€â”€ main_agent.py
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â””â”€â”€ covers/   # AI images land here
â”‚   â””â”€â”€ .env          # â† you create this
â”œâ”€â”€ frontend/         # CRA React app (noÂ Vite)
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ package.json
â””â”€â”€ environment.yml   # single source of dependency truth
```

`__pycache__` is ignored via `.gitignore`.\

---

## 5Â Â·Â SecretsÂ Â·Â `.env` (oneâ€‘time)

1. **Switch to the backend folder**  
   ```bash
   cd backend
   ```

2. **Create the file** (interactive *heredoc*; hitÂ `ENTER`, paste your keys, thenÂ `CTRLâ€‘D`):

   ```bash
   cat > .env <<'ENV'
   OPENAI_API_KEY=sk-________________________________________
   PINECONE_API_KEY=pc-______________________________________
   JUDGMENT_API_KEY=jv-______________________________________
   JUDGMENT_ORG_ID=org-_____________________________________
   ENV
## 6Â Â·Â Run the Backend Server

```bash
# from repo root
conda activate prose-art-gen           # if not already
python backend/main_agent.py
```

You should see:

```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

The server autoâ€‘reloads on code edits (`uvicorn --reload` inside the script).

---

## 7Â Â·Â Smokeâ€‘Test the Endpoint

```bash
curl -X POST http://localhost:8000/generate-prose \
     -H "Content-Type: application/json" \
     -d '{
          "beats": [
            "Jack and Xander continue their excavation on the lunar surface.",
            "They discover an ancient alien artifact."
          ],
          "characters": [
            {"name": "Jack",   "description": "A veteran lunar miner"},
            {"name": "Xander", "description": "Jack's logical partner"}
          ],
          "setting": "A secluded lunar excavation site overshadowed by corporate exploitation",
          "genre":   "Hard Sci-Fi",
          "style":   "Dark, suspenseful atmosphere reminiscent of Alien",
          "cover_style": "oil_paint",
          "visual_prompt": "show the artifact pulsing with blue light"
        }' | jq
```

Expected response:

```jsonc
{
  "prose_output": "<â‰ˆ1 500-word passage>",
  "cover_image": "/static/covers/abcd1234.png"
}
```

Open `http://localhost:8000/static/covers/abcd1234.png` to view the illustration.

---

## 8Â Â·Â Frontend (ReactÂ 18, CRA)


```bash
cd frontend
npm install         # installs ReactÂ 18, axios, etc.
npm install @mui/material @mui/icons-material @emotion/react @emotion/styled # Install Material UI core and icon
npm start           # http://localhost:3000
```

The CRA devâ€‘server proxies API calls to `:8000` (see `package.json â†’ proxy`).\
Feel free to migrate to Next.js or another framework, but remember to update this README.


---

Made with â˜„ï¸Â **LangGraph** Â· âš›ï¸Â **React** Â· ğŸ”Â **Pinecone** Â· ğŸ¨Â **OpenAI GPTâ€‘Image** Â· ğŸ”¬Â **Judgeâ€‘val**.

